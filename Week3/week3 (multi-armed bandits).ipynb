{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Assignment\n",
    "## Multi Armed Bandits\n",
    "\n",
    "Let's get into some coding for this. \n",
    "Multi-Armed Bandits are fun because you get a bunch of interesting and intuitive algorithms for solving a straightforward task: figure out the distribution of the rewards for the arms and thus be able to compare and figure out the best arm.\n",
    "\n",
    "### Tasks\n",
    "- Using `epsilon_greedy` as a reference (complete code is provided), write code in `klucb`, `ucb` and `thompson` so that agents following that algorithm can play the game correctly.\n",
    "- Each agent is derived from the baseclass in the first code block. Please don't make changes to it\n",
    "\n",
    "### Some Points on Assignment Structure\n",
    "- Here we have tried to give you a brief insight into how object oriented programming works. This is very useful and allows for programs to be modular and thus reused and replaced as and when necesssary without hassle.\n",
    "- This shouldn't pose an issue in the logic building, but in case it does, or you have related doubts, feel free to drop us a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't make any changes to this\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, arms : np.ndarray[float]):\n",
    "        self.arms = arms # list of probabilities of each arm returning a favorable reward\n",
    "        self.best_arm = np.max(arms) # useful for regret calculation\n",
    "        self.cumulative_regret_array = [0]\n",
    "\n",
    "    def pull(self, arm:int) -> int:\n",
    "        assert arm in np.arange(0, len(self.arms)), \"Action undefined for bandit\"\n",
    "        reward = 1 if np.random.random() < self.arms[arm] else 0\n",
    "        self.cumulative_regret_array.append(self.cumulative_regret_array[-1] + self.best_arm - reward)\n",
    "        return reward\n",
    "    \n",
    "    def plot_cumulative_regret(self):\n",
    "        timesteps = np.arange(1, len(self.cumulative_regret_array) + 1)\n",
    "\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(timesteps, self.cumulative_regret_array, linestyle='-', color='r', label='Cumulative Regret')\n",
    "\n",
    "        # Formatting\n",
    "        plt.title('Cumulative Regret Over Time', fontsize=16)\n",
    "        plt.xlabel('Timesteps', fontsize=14)\n",
    "        plt.ylabel('Cumulative Regret', fontsize=14)\n",
    "        plt.grid(True, which='both', linestyle='-', linewidth=0.5)\n",
    "        plt.yticks(np.arange(0, max(self.cumulative_regret_array) + 5, step=5))\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(loc='upper left', fontsize=12)\n",
    "\n",
    "        # Tight layout to ensure there's no clipping of labels\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, time_to_run, bandit : MultiArmedBandit):\n",
    "        self.time_to_run = time_to_run\n",
    "        self.rewards = []\n",
    "        self.bandit = bandit\n",
    "        self.arms = len(bandit.arms)\n",
    "    \n",
    "    def plot_reward_vs_time_curve(self):\n",
    "\n",
    "        # Create an index for timesteps\n",
    "        timesteps = np.arange(1, len(self.rewards) + 1)\n",
    "\n",
    "        # Average out self.rewards\n",
    "        avg_rewards = [np.mean(self.rewards[0:T+1]) for T in range(self.time_to_run)]\n",
    "\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(timesteps, avg_rewards, linestyle='-', color='g', label='Rewards')\n",
    "\n",
    "        # Formatting\n",
    "        plt.title('Average Reward Over Time', fontsize=16)\n",
    "        plt.xlabel('Timesteps', fontsize=14)\n",
    "        plt.ylabel('Mean Reward Value upto timestep t', fontsize=14)\n",
    "        plt.grid(True, which='both', linestyle='-', linewidth=0.5)\n",
    "        # plt.xticks(timesteps)  # Show all timesteps as x-axis ticks\n",
    "        # plt.yticks(np.arange(0, max(self.rewards) + 5, step=5))\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(loc='upper left', fontsize=12)\n",
    "\n",
    "        # Tight layout to ensure there's no clipping of labels\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is properly written (just need to run)\n",
    "\n",
    "class EpsilonGreedyAgent(Agent):\n",
    "    epsilon : float\n",
    "    reward_memory : np.ndarray # A per arm value of how much reward was gathered\n",
    "    count_memory : np.ndarray # An array of the number of times an arm is pulled \n",
    "\n",
    "    def __init__(self, time_horizon, bandit:MultiArmedBandit, epsilon = 0.01):\n",
    "        super().__init__(time_horizon, bandit)\n",
    "        self.epsilon = epsilon\n",
    "        self.bandit : MultiArmedBandit = bandit\n",
    "        self.reward_memory = np.zeros(len(bandit.arms))\n",
    "        self.count_memory = np.zeros(len(bandit.arms))\n",
    "        self.time_step = 0\n",
    "\n",
    "    def give_pull(self):\n",
    "        if np.random.random() < self.epsilon: # Choose random action\n",
    "           random_arm = np.random.choice(len(self.bandit.arms))\n",
    "           reward =  self.bandit.pull(random_arm)\n",
    "           self.reinforce(reward, random_arm)\n",
    "           self.count_memory[random_arm] += 1\n",
    "           self.reward_memory[random_arm] += reward\n",
    "        else: # Choose best known action!\n",
    "           best_arm = np.argmax(self.reward_memory / self.count_memory)\n",
    "           reward = self.bandit.pull(best_arm)\n",
    "           self.reinforce(reward, best_arm)\n",
    "           \n",
    "\n",
    "    def reinforce(self, reward, arm):\n",
    "        self.count_memory[arm] += 1\n",
    "        self.reward_memory[arm] += reward\n",
    "        self.time_step += 1\n",
    "        self.rewards.append(reward)\n",
    "        # print(self.count_memory, self.reward_memory)\n",
    " \n",
    "    def plot_arm_graph(self):\n",
    "        counts = self.count_memory\n",
    "        indices = np.arange(len(counts))\n",
    "\n",
    "        # Plot the data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(indices, counts, color='skyblue', edgecolor='black')\n",
    "\n",
    "        # Formatting\n",
    "        plt.title('Counts per Category', fontsize=16)\n",
    "        plt.xlabel('Arm', fontsize=14)\n",
    "        plt.ylabel('Pull Count', fontsize=14)\n",
    "        plt.grid(axis='y', linestyle='-')  # Add grid lines for the y-axis\n",
    "        plt.xticks(indices, [f'Category {i+1}' for i in indices], rotation=45, ha='right')\n",
    "        # plt.yticks(np.arange(0, max(counts) + 2, step=2))\n",
    "\n",
    "        # Annotate the bars with the count values\n",
    "        for i, count in enumerate(counts):\n",
    "            plt.text(i, count + 0.5, str(count), ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "        # Tight layout to ensure there's no clipping of labels\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test\n",
    "# Init Bandit\n",
    "TIME_HORIZON = 10_000\n",
    "bandit = MultiArmedBandit(np.array([0.23,0.55,0.76,0.44]))\n",
    "agent = EpsilonGreedyAgent(TIME_HORIZON, bandit, 0.05)\n",
    "\n",
    "# Loop\n",
    "for i in range(TIME_HORIZON):\n",
    "    agent.give_pull()\n",
    "\n",
    "# Plot curves\n",
    "agent.plot_reward_vs_time_curve()\n",
    "agent.plot_arm_graph()\n",
    "bandit.plot_cumulative_regret()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent(Agent):\n",
    "    # Add fields \n",
    "\n",
    "    def __init__(self, time_horizon, bandit:MultiArmedBandit,): \n",
    "        # Add fields\n",
    "        super().__init__(time_horizon, bandit)\n",
    "\n",
    "    def give_pull(self):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    def reinforce(self, reward, arm):\n",
    "        raise NotImplementedError\n",
    " \n",
    "    def plot_arm_graph(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test\n",
    "# Init Bandit\n",
    "TIME_HORIZON = 10_000\n",
    "bandit = MultiArmedBandit(np.array([0.23,0.55,0.76,0.44]))\n",
    "agent = None ## Fill with correct constructor\n",
    "\n",
    "# Loop\n",
    "for i in range(TIME_HORIZON):\n",
    "    agent.give_pull()\n",
    "\n",
    "# Plot curves\n",
    "agent.plot_reward_vs_time_curve()\n",
    "agent.plot_arm_graph()\n",
    "bandit.plot_cumulative_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLUCBAgent(Agent):\n",
    "    # Add fields \n",
    "\n",
    "    def __init__(self, time_horizon, bandit:MultiArmedBandit,): \n",
    "        # Add fields\n",
    "        super().__init__(time_horizon, bandit)\n",
    "\n",
    "    def give_pull(self):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    def reinforce(self, reward, arm):\n",
    "        raise NotImplementedError\n",
    " \n",
    "    def plot_arm_graph(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test\n",
    "# Init Bandit\n",
    "TIME_HORIZON = 10_000\n",
    "bandit = MultiArmedBandit(np.array([0.23,0.55,0.76,0.44]))\n",
    "agent = None ## Fill with correct constructor\n",
    "\n",
    "# Loop\n",
    "for i in range(TIME_HORIZON):\n",
    "    agent.give_pull()\n",
    "\n",
    "# Plot curves\n",
    "agent.plot_reward_vs_time_curve()\n",
    "agent.plot_arm_graph()\n",
    "bandit.plot_cumulative_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent(Agent):\n",
    "    # Add fields \n",
    "\n",
    "    def __init__(self, time_horizon, bandit:MultiArmedBandit,): \n",
    "        # Add fields\n",
    "        super().__init__(time_horizon, bandit)\n",
    "\n",
    "    def give_pull(self):\n",
    "        raise NotImplementedError \n",
    "\n",
    "    def reinforce(self, reward, arm):\n",
    "        raise NotImplementedError\n",
    " \n",
    "    def plot_arm_graph(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test\n",
    "# Init Bandit\n",
    "TIME_HORIZON = 10_000\n",
    "bandit = MultiArmedBandit(np.array([0.23,0.55,0.76,0.44]))\n",
    "agent = None ## Fill with correct constructor\n",
    "\n",
    "# Loop\n",
    "for i in range(TIME_HORIZON):\n",
    "    agent.give_pull()\n",
    "\n",
    "# Plot curves\n",
    "agent.plot_reward_vs_time_curve()\n",
    "agent.plot_arm_graph()\n",
    "bandit.plot_cumulative_regret()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's required in `main` ?\n",
    "Once all agents are ready, let them all try their hands at the same games (each run should result the same reward sequence for every agent playing the game). \n",
    "\n",
    "The first set of games, S1, will be vanilla (again 30k time steps). This set just contains the single bandit game given in the main section of any of those files you saw. Run that instance and plot a single curve (one for cumulative regret, one for cumulative reward) for all the 4 agents. Also plot a bar chart of the number of times each agent chose the optimal arm alone.\n",
    "\n",
    "\n",
    "The next set of games we'll call S2 include a pair of bandits of probability $p$ and $p+0.1$ where $p \\in$ \\{0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9\\}. I.e., in each game, there'll be two bandits, and there'll be 17 such games. For S2, cumulative reward and regret curves are not required to be plotted. What will be required is a plot of the final regret at the end of the games for each value of $p$. This should be easy. The hard part is to explain why the curves you obtained for KL-UCB and UCB look the way they look. Explain the answer to this riddle in a markdown cell at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "# Write code which will run all the different bandit agents together and:\n",
    "# 1. Plot a common cumulative regret curves graph\n",
    "# 2. Plot a common graph of average reward curves\n",
    "\n",
    "# Solve S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
