{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Solving Blackjack with TD-Learning and Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we’ll explore and solve the *Blackjack-v1*\n",
        "environment.\n",
        "\n",
        "**Blackjack** is one of the most popular casino card games that is also\n",
        "infamous for being beatable under certain conditions. This version of\n",
        "the game uses an infinite deck (we draw the cards with replacement), so\n",
        "counting cards won’t be a viable strategy in our simulated game.\n",
        "Full documentation can be found at https://gymnasium.farama.org/environments/toy_text/blackjack\n",
        "\n",
        "Checkout some gameplay without registration at: https://www.arkadium.com/games/blackjack/\n",
        "\n",
        "\n",
        "### Blackjack Rules\n",
        "\n",
        "1. **Objective**: The goal is to have a card sum greater than the dealer's without exceeding 21.\n",
        "2. **States**: Each state is defined by:\n",
        "   - The player's current sum (12-21).\n",
        "   - The dealer's showing card (ace-10).\n",
        "   - Whether the player has a usable ace.\n",
        "3. **Rewards**: \n",
        "   - +1 for winning.\n",
        "   - 0 for a draw.\n",
        "   - -1 for losing.\n",
        "4. **Actions**: The player can either \"hit\" (take another card) or \"stick\" (stop taking cards).\n",
        "5. **Gameplay**: \n",
        "   - The player is dealt two cards initially and can choose to hit or stick.\n",
        "   - The dealer then reveals one of their cards and follows a fixed strategy.\n",
        "   - The game ends when the player sticks or exceeds 21, and the winner is determined based on the final sums.\n",
        "\n",
        "\n",
        "**Approach**: To solve this environment by yourself, implement the TD(λ) algorithm. Change the values of λ to observe the varying behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Environment Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "# Let's start by creating the blackjack environment.\n",
        "# Note: We are going to follow the rules from Sutton & Barto.\n",
        "# Other versions of the game can be found below for you to experiment.\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\", sab=True, render_mode='human')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observing the environment\n",
        "\n",
        "First of all, we call ``env.reset()`` to start an episode. This function\n",
        "resets the environment to a starting position and returns an initial\n",
        "``observation``. We usually also set ``done = False``. This variable\n",
        "will be useful later to check if a game is terminated (i.e., the player wins or loses).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
          ]
        }
      ],
      "source": [
        "# reset the environment to get the first observation\n",
        "done = False\n",
        "observation, info = env.reset()\n",
        "\n",
        "# observation = (16, 9, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that our observation is a 3-tuple consisting of 3 values:\n",
        "\n",
        "-  The players current sum\n",
        "-  Value of the dealers face-up card\n",
        "-  Boolean whether the player holds a usable ace (An ace is usable if it\n",
        "   counts as 11 without busting)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing an action\n",
        "\n",
        "After receiving our first observation, we are only going to use the\n",
        "``env.step(action)`` function to interact with the environment. This\n",
        "function takes an action as input and executes it in the environment.\n",
        "Because that action changes the state of the environment, it returns\n",
        "four useful variables to us. These are:\n",
        "\n",
        "-  ``next_state``: This is the observation that the agent will receive\n",
        "   after taking the action.\n",
        "-  ``reward``: This is the reward that the agent will receive after\n",
        "   taking the action.\n",
        "-  ``terminated``: This is a boolean variable that indicates whether or\n",
        "   not the environment has terminated.\n",
        "-  ``truncated``: This is a boolean variable that also indicates whether\n",
        "   the episode ended by early truncation, i.e., a time limit is reached.\n",
        "-  ``info``: This is a dictionary that might contain additional\n",
        "   information about the environment.\n",
        "\n",
        "The ``next_state``, ``reward``,  ``terminated`` and ``truncated`` variables are\n",
        "self-explanatory, but the ``info`` variable requires some additional\n",
        "explanation. In the Blackjack-v1 environment you can ignore it. \n",
        "\n",
        "Note that it is not a good idea to call ``env.render()`` in your training\n",
        "loop because rendering slows down training by a lot. Rather try to build\n",
        "an extra loop to evaluate and showcase the agent after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sample a random action from all valid actions\n",
        "action = env.action_space.sample()\n",
        "# action=1\n",
        "\n",
        "# execute the action in our environment and receive infos from the environment\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "# observation=(24, 10, False)\n",
        "# reward=-1.0\n",
        "# terminated=True\n",
        "# truncated=False\n",
        "# info={}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once ``terminated = True`` or ``truncated=True``, we should stop the\n",
        "current episode and begin a new one with ``env.reset()``. If you\n",
        "continue executing actions without resetting the environment, it still\n",
        "responds but the output won’t be useful for training (it might even be\n",
        "harmful if the agent learns on invalid data).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()\n",
        "# we close the environment since we want to train it without rendering, else it will be super slow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation of Monte Carlo, TD-Learning, and TD(λ)\n",
        "\n",
        "**Monte Carlo Algorithm**:\n",
        "Imagine you’re playing a game and you decide to learn from it only after it’s finished. That's Monte Carlo! It waits until the end of an episode, then takes the total reward and updates the values based on what happened throughout the whole game. It’s great because it uses the entire episode to make decisions, but sometimes it can be a bit wild with its estimates because it’s all-or-nothing.\n",
        "\n",
        "**TD-Learning (Temporal Difference Learning)**:\n",
        "Now, what if we don't want to wait until the end of the episode? Enter TD-Learning! It updates values step by step, using the immediate reward and the estimate of the next state. Think of it as learning on the fly. The classic one-step TD, or TD(0), updates after each step which helps in stabilizing the learning process. It’s less chaotic compared to Monte Carlo but can sometimes be biased because it’s just looking one step ahead.\n",
        "\n",
        "**TD(λ)**:\n",
        "Here’s where things get interesting. TD(λ) is like the best of both worlds. By adjusting the parameter λ (lambda), we can find the sweet spot between Monte Carlo and one-step TD. \n",
        "- Set λ to 1, and, you’ve got Monte Carlo learning from full episodes.\n",
        "- Set λ to 0, and hey, it’s one-step TD all over again.\n",
        "- With λ between 0 and 1, TD(λ) blends these approaches, balancing long-term and short-term learning.\n",
        "\n",
        "### Why TD(λ)?\n",
        "By implementing TD(λ), we get a lot more flexibility. We can make more frequent updates without waiting for an episode to end, and we can factor in multiple future steps to make our estimates better. Essentially, TD(λ) helps us balance between the extreme detailed look-ahead and the broader long-term returns. The theory in Grokking goes in depth into this and talks about the bias-variance trade-off, so, do check that out too.\n",
        "\n",
        "### Time to Experiment\n",
        "Experiment with different values of λ and see how your agent learns differently. Try λ at 0, 0.5, 0.9, 1, and note the changes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building an agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlackjackTDAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate: float,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        discount_factor: float = 0.95,\n",
        "        lambda_: float = 0.9  # Adjustable lambda for TD(lambda)\n",
        "    ):\n",
        "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
        "        of state-action values (q_values), a learning rate, epsilon, and lambda.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: The learning rate\n",
        "            initial_epsilon: The initial epsilon value\n",
        "            epsilon_decay: The decay for epsilon\n",
        "            final_epsilon: The final epsilon value\n",
        "            discount_factor: The discount factor for computing the Q-value\n",
        "            lambda_: The decay rate for eligibility traces\n",
        "        \"\"\"\n",
        "        # Initialize Q-values and eligibility traces\n",
        "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "        self.eligibility_traces = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "        # Set hyperparameters\n",
        "        self.lr = \n",
        "        self.discount_factor = \n",
        "        self.lambda_ = \n",
        "\n",
        "        self.epsilon = \n",
        "        self.epsilon_decay = \n",
        "        self.final_epsilon = \n",
        "\n",
        "        self.training_error = []\n",
        "\n",
        "    def get_action(self, env, obs: tuple[int, int, bool]) -> int:\n",
        "        \"\"\"\n",
        "        Returns the best action with probability (1 - epsilon)\n",
        "        otherwise a random action with probability epsilon to ensure exploration.\n",
        "        \"\"\"\n",
        "        # Implement epsilon-greedy action selection here\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        obs: tuple[int, int, bool],\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: tuple[int, int, bool],\n",
        "        next_action: int\n",
        "    ):\n",
        "        \"\"\"Updates the Q-value of an action using TD(lambda).\"\"\"\n",
        "        # Calculate the future Q-value based on whether the state is terminal\n",
        "        future_q_value = \n",
        "        \n",
        "        # Calculate the temporal difference error\n",
        "        temporal_difference = \n",
        "\n",
        "        # Update eligibility trace for the current state-action pair\n",
        "\n",
        "        # Update all Q-values and eligibility traces\n",
        "        for state, actions in self.q_values.items():\n",
        "            for a in range(len(actions)):\n",
        "                # Update Q-value based on temporal difference and eligibility trace\n",
        "\n",
        "                \n",
        "                # Decay the eligibility trace\n",
        "\n",
        "\n",
        "        # Record the training error\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
        "\n",
        "# Usage Example (to be filled in by students):\n",
        "# Create the Blackjack environment\n",
        "# env = gym.make('Blackjack-v0')\n",
        "# Initialize the agent\n",
        "# agent = BlackjackAgent(env, learning_rate=0.1, initial_epsilon=1.0, epsilon_decay=0.01, final_epsilon=0.1, discount_factor=0.95, lambda_=0.9)\n",
        "# Implement the training loop (to be filled in by students)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now make the environment for training without rendering\n",
        "env = gym.make(\"Blackjack-v1\", sab=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``n_episodes`` needs to be changed to a large value (remove the comment to train, after you have verified) to have proper amount of time to learn.\n",
        "It is set to a small value initially just for you to check if your code is right, because afterwards, it will run for a long time to train.  \n",
        "\n",
        "``learning_rate`` can also be tinkered with to control how fast the agent learns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.001\n",
        "\n",
        "# n_episodes = 1000_000\n",
        "n_episodes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start_epsilon = 0.99\n",
        "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
        "final_epsilon = 0.05\n",
        "\n",
        "# change this value to observe different systems of TD learning\n",
        "lambda_ = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = BlackjackTDAgent(\n",
        "    env=env,\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        "    lambda_= lambda_\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # play one episode\n",
        "    while not done:\n",
        "        action = agent.get_action(env, obs)\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        next_action = agent.get_action(env, next_obs)\n",
        "        # update the agent\n",
        "        agent.update(obs, action, reward, terminated, next_obs, next_action)\n",
        "\n",
        "        # update if the environment is done and the current obs\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "\n",
        "    agent.decay_epsilon()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the training\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rolling_length = 500\n",
        "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
        "axs[0].set_title(\"Episode rewards\")\n",
        "# compute and assign a rolling average of the data to provide a smoother graph\n",
        "reward_moving_average = (\n",
        "    np.convolve(\n",
        "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
        "    )\n",
        "    / rolling_length\n",
        ")\n",
        "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
        "axs[1].set_title(\"Episode lengths\")\n",
        "length_moving_average = (\n",
        "    np.convolve(\n",
        "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
        "    )\n",
        "    / rolling_length\n",
        ")\n",
        "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
        "axs[2].set_title(\"Training Error\")\n",
        "training_error_moving_average = (\n",
        "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
        "    / rolling_length\n",
        ")\n",
        "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualising the policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_grids(agent, usable_ace=False):\n",
        "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
        "    # convert our state-action values to state values\n",
        "    # and build a policy dictionary that maps observations to actions\n",
        "    state_value = defaultdict(float)\n",
        "    policy = defaultdict(int)\n",
        "    for obs, action_values in agent.q_values.items():\n",
        "        state_value[obs] = float(np.max(action_values))\n",
        "        policy[obs] = int(np.argmax(action_values))\n",
        "\n",
        "    player_count, dealer_count = np.meshgrid(\n",
        "        # players count, dealers face-up card\n",
        "        np.arange(12, 22),\n",
        "        np.arange(1, 11),\n",
        "    )\n",
        "\n",
        "    # create the value grid for plotting\n",
        "    value = np.apply_along_axis(\n",
        "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
        "        axis=2,\n",
        "        arr=np.dstack([player_count, dealer_count]),\n",
        "    )\n",
        "    value_grid = player_count, dealer_count, value\n",
        "\n",
        "    # create the policy grid for plotting\n",
        "    policy_grid = np.apply_along_axis(\n",
        "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
        "        axis=2,\n",
        "        arr=np.dstack([player_count, dealer_count]),\n",
        "    )\n",
        "    return value_grid, policy_grid\n",
        "\n",
        "\n",
        "def create_plots(value_grid, policy_grid, title: str):\n",
        "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
        "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
        "    player_count, dealer_count, value = value_grid\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # plot the state values\n",
        "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
        "    ax1.plot_surface(\n",
        "        player_count,\n",
        "        dealer_count,\n",
        "        value,\n",
        "        rstride=1,\n",
        "        cstride=1,\n",
        "        cmap=\"viridis\",\n",
        "        edgecolor=\"none\",\n",
        "    )\n",
        "    plt.xticks(range(12, 22), range(12, 22))\n",
        "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
        "    ax1.set_title(f\"State values: {title}\")\n",
        "    ax1.set_xlabel(\"Player sum\")\n",
        "    ax1.set_ylabel(\"Dealer showing\")\n",
        "    ax1.zaxis.set_rotate_label(False)\n",
        "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
        "    ax1.view_init(20, 220)\n",
        "\n",
        "    # plot the policy\n",
        "    fig.add_subplot(1, 2, 2)\n",
        "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
        "    ax2.set_title(f\"Policy: {title}\")\n",
        "    ax2.set_xlabel(\"Player sum\")\n",
        "    ax2.set_ylabel(\"Dealer showing\")\n",
        "    ax2.set_xticklabels(range(12, 22))\n",
        "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
        "\n",
        "    # add a legend\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
        "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
        "    ]\n",
        "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
        "    return fig\n",
        "\n",
        "\n",
        "# state values & policy with usable ace (ace counts as 11)\n",
        "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
        "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# state values & policy without usable ace (ace counts as 1)\n",
        "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
        "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Think your agent works?\n",
        "\n",
        "Run the following function to see the fruits of your hardwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def watch_agent_play(agent, n_episodes=5, delay=1.0):\n",
        "    \"\"\"\n",
        "    Function to watch the agent play the game for a given number of episodes with a delay between steps.\n",
        "    \n",
        "    Args:\n",
        "        agent: The trained Blackjack agent\n",
        "        env_name: The name of the environment\n",
        "        n_episodes: Number of episodes to watch\n",
        "        delay: Time delay (in seconds) between each step\n",
        "    \"\"\"\n",
        "    env = gym.make(\"Blackjack-v1\", sab=True, render_mode='human')\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        \n",
        "        print(f\"Episode {episode + 1}\\n{'-'*20}\")\n",
        "        \n",
        "        while not done:\n",
        "            env.render()\n",
        "            time.sleep(delay)\n",
        "            \n",
        "            action = agent.get_action(env, obs)\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            \n",
        "            if action == 0:\n",
        "                print(\"Agent decides to HIT\")\n",
        "            else:\n",
        "                print(\"Agent decides to STICK\")\n",
        "            \n",
        "            done = terminated or truncated\n",
        "            obs = next_obs\n",
        "            episode_reward += reward\n",
        "            \n",
        "            # Print the current situation\n",
        "            print(f\"Current Observation: {obs}\")\n",
        "            print(f\"Current Reward: {episode_reward}\\n\")\n",
        "            \n",
        "            time.sleep(delay)\n",
        "        \n",
        "        if episode_reward > 0:\n",
        "            result = \"WIN\"\n",
        "        elif episode_reward < 0:\n",
        "            result = \"LOSE\"\n",
        "        else:\n",
        "            result = \"DRAW\"\n",
        "        \n",
        "        print(f\"Total Reward: {episode_reward} - Result: {result}\\n\")\n",
        "        print(f\"{'='*20}\")\n",
        "    \n",
        "    env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run this function to watch your agent work its magic!\n",
        "watch_agent_play(agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### That's it! Hope it was fun"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gpu_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
